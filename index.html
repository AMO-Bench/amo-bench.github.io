<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="AMO-Bench: Large Language Models Still Struggle in High School Math Competitions">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="AMO-Bench is an advanced mathematical reasoning benchmark with olympiad level or even higher difficulty, comprising 50 human-crafted problems.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Large Language Models, Reasoning Models, Benchmark, Mathematical Reasoning">
  <!-- TODO: List all authors -->
  <meta name="author" content="Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Shuang Zhou (Alphabetical order by last name)">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Meituan LongCat Team">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="AMO-Bench: Large Language Models Still Struggle in High School Math Competitions">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="AMO-Bench is an advanced mathematical reasoning benchmark with olympiad level or even higher difficulty, comprising 50 human-crafted problems.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://amo-bench.github.io">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://vitabench.github.io/static/images/main_result.pdf">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="604">
  <meta property="og:image:alt" content="Overview of the AMO-Bench construction pipeline.">
  <meta property="article:published_time" content="2025-10-16T00:00:00.000Z">
  <meta property="article:author" content="Shengnan An, Junlin Liu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Large Language Models">
  <meta property="article:tag" content="Mathematical Reasoning">


  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title> AMO-Bench: Large Language Models Still Struggle in High School Math Competitions </title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

</head>

<body>
  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- TODO: Replace with your paper title -->
              <h1 class="title is-1 publication-title"> üìê AMO-Bench: Large Language Models Still <br> 
                Struggle in High School Math Competitions</h1>
              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your paper authors and their personal links -->
                <span class="author-block">
                  Shengnan An<sup>*‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Xunliang Cai<sup>‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Xuezhi Cao<sup>*‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Xiaoyu Li<sup>‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Yehao Lin<sup>‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Junlin Liu<sup>‚Ä†‚ô£</sup>,
                </span>
                <br>
                <span class="author-block">
                  Xinxuan Lv<sup>‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Dan Ma<sup>‚ô¢</sup>,
                </span>
                <span class="author-block">
                  Xuanlin Wang<sup>‚Ä†‚ô°</sup>,
                </span>
                <span class="author-block">
                  Shuang Zhou<sup>‚ô¢</sup>,
                </span>
                <br>
                <span class="author-block">
                  (Alphabetical order by last name)<sup></sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <!-- TODO: Replace with your institution and conference/journal info -->
                <span class="author-block">
                  <a href="https://github.com/meituan-longcat" target="_blank">Meituan LongCat Team</a>
                </span>
                <br>
                <span class="author-block">
                  <small>*Correspondence to: <code>anshengnan@meituan.com</code>, <code>caoxuezhi@meituan.com</code>.</small>
                </span>
                <br>
                <span class="author-block">
                  <small><sup>‚ô¢</sup>Meituan</small>
                </span>
                <br>
                <span class="author-block">
                  <small><sup>‚ô£</sup>University of Chinese Academy of Sciences</small>
                </span>
                <br>
                <span class="author-block">
                  <small><sup>‚ô°</sup>Harbin Institute of Technology</small>
                </span>
                <br>
                <span class="author-block">
                  <small><sup>‚Ä†</sup>Work done during the internship at Meituan.</small>
                </span>
                <!-- TODO: Remove this line if no equal contribution -->
                <!-- <span class="eql-cntrb"><small><br>The first six authors are core contributors.</small></span> -->
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2509.26490" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        üìÉ
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  
                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/meituan-longcat/vitabench" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  
                  
                  <!-- Dataset -->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/meituan-longcat/VitaBench" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        ü§ó
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  
                  <!-- Leaderboard ÈìæÊé• -->
                  <span class="link-block">
                    <a href="#Leaderboard"
                      class="button is-normal is-rounded is-dark">
                      <span class="icon">
                        üèÜ
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <!-- TODO: Replace with your paper abstract -->
              <p>
                We present <b>AMO-Bench</b>, an <b><u>A</u></b>dvanced <b><u>M</u></b>athematical reasoning benchmark with <b><u>O</u></b>lympiad level or even higher difficulty, comprising 50 human-crafted problems. 
                Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). 
                However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). 
                To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are 
                (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and 
                (2) entirely original problems to prevent potential performance leakages from data memorization. 
                Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. 
                Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. 
                Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. 
                These results highlight the significant room for improving the mathematical reasoning in current LLMs. 
                We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models.
              </p>
              <p style="text-align: center;">
                <img src="static/images/main_result.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 90%; height: auto;" loading="lazy" />
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <hr class="gray-separator" style="background-color: #bbb !important;">
    <!-- End paper abstract -->
    

    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <div class="content" style="padding-left:10%; padding-right:10%">
              <h2 class="title is-3">Dataset Statistics</h2>
              <div class="level-set has-text-justified">
                <p>
                  <b>A.Problem categories:</b> Referring several official competition syllabus, we categorize the 50 problems of AMO-Bench into the following five primary categories: 
                  Algebraic Equations & Inequalities (11/50), Functions & Sequences (13/50), Geometry (5/50), Number Theory (9/50), and Combinatorics (12/50). 
                  Figure a show the overall distribution of problem categories in AMO-Bench.
                  <br>

                  <b>B.Length distribution of human-annotated solutions:</b> 
                  Since the problems in our AMO-Bench are equipped with manually annotated solutions, we can preliminarily analyze the reasoning complexity of these problems from the view of solution length. 
                  We measure solution length in terms of token count. Additionally, we compare the distribution of solution lengths with those from AIME24 and MATH500. 
                  Figure b illustrates the solution length distributions across these benchmarks. 
                  It reveals that solutions in AMO-Bench exhibit significantly higher lengths, indicating that problems in this benchmark are inherently more challenging and require more complex reasoning to arrive at the final answer. 
                </p>
                <p style="text-align: center;">
                   <img src="static/images/data.png" alt="Sizes of model trees" class="blend-img-background center-image" style="max-width: 90%; height: auto; padding-left:5%; padding-right:5%" loading="lazy" />
                </p>
              </div>
          </div>
        </div>
      </div>
    </section>

    <hr class="gray-separator" style="background-color: #bbb !important;">
    
    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <div class="content" style="padding-left:10%; padding-right:10%">
              <h2 class="title is-3">Construction and Grading Pipeline</h2>
              <div class="level-set has-text-justified">
                  <p>
                    <b>A.Construction pipeline</b>: AMO-Bench have built up a comprehensive multi-stage construction pipeline that covers the entire process from question creation to final inclusion. 
                    This pipeline comprises four major stages: 
                    <b>(1)Data creation, </b>
                    all problems are independently designed by mathematics experts from top universities and educational institutions. 
                    Beyond the final answer, each problem author must provide a detailed step-by-step solution.
                    <b>(2)Quality review, </b>
                    each candidate problem undergoes blind review by at least three experts to assess its quality. 
                    <b>(3)Originality review, </b>
                    the originality review stage aims to ensure that these newly created problems are not mere rewrites of publicly available materials, but demonstrate genuine originality.
                    <b>(4)Difficulty review, </b>
                    we implement a difficulty review stage to filter out problems lacking adequate complexity, to ensure that AMO-Bench presents a sufficient challenge to state-of-the-art LLMs.
                  </p>
                <br>
                  <p>
                    <b>B.Grading Pipeline</b>: AMO-Bench employs different grading approaches based on the specific answer type for each problem. 
                     For problems requiring numerical, set, or variable-expression answers (39 out of 50), we employ the <b>parser-based grading</b>. 
                    For problems requiring descriptive answers (11 out of 50), we use <b>LLM-based grading</b> with o4-mini (Low) serving as the grading model.
                  </p>
                  
                  <img src="static/images/pipeline.png" alt="Overview of the VitaBench construction pipeline" class="blend-img-background center-image" style="max-width: 90%; height: auto; display: block; margin: 0 auto; paddng-top:2%" loading="lazy" />

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <hr class="gray-separator" style="background-color: #bbb !important;">


  <section class="section hero is-small" id="Leaderboard">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <div class="content" style="padding-left:10%; padding-right:10%">
              <h2 class="title is-3">üèÜLeaderboardüèÜ</h2>
              <div class="level-set has-text-justified">
                <p>
                  This figure summarizes the AVG@32 performance of various leading LLMs, categorized by proprietary/open-source status and reasoning/non-reasoning properties. 
                  Even the highest performing model GPT-5-Thinking (High) reaches just <strong>52.4%</strong>, while most others score below <strong>40%</strong>. 
                  This indicates substantial room for improvement in complex reasoning abilities across all current language models. 
                  Moreover, both proprietary and open-source reasoning models occupy top ranks in the leaderboard, indicating that recent open-source advancements are closing the gap with leading commercial models. 
                </p>
                <p>
                    <img src="static/images/AMO_result.png" alt="Sizes of model trees" class="blend-img-background center-image" style="max-width: 95%; height: auto;" loading="lazy" />
                </p>
              </div>
          </div>
        </div>
      </div>
    </section>




    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code">
          <code>@article{he2025vitabench,
                title={VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications}, 
                author={He, Wei and Sun, Yueqing and Hao, Hongyan and Hao, Xueyuan and Xia, Zhikang and Gu, Qi and Han, Chengcheng and Zhao, Dengchang and Su, Hui and Zhang, Kefeng and Gao, Man and Su, Xi and Cai, Xiaodong and Cai, Xunliang and Yang, Yu and Zhao, Yunke},
                journal={arXiv preprint arXiv:2509.26490},
                year={2025}
                }
          </code>
        </pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>


</html>













































